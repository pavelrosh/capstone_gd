{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "import shutil\n",
    "import unittest\n",
    "from time import time\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import MapType\n",
    "from pyspark.sql.functions import udf, col, array, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Build Purchases Attribution Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.31.212:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10ea9b9d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickstream_df = spark.read.csv(sep=r'\\t', path='input_csv_datasets/mobile-app-clickstream_sample.tsv', header=True)\n",
    "clickstream_df.createOrReplaceTempView('clickstream')\n",
    "clickstream_df = spark.sql(\"SELECT * FROM clickstream \"\n",
    "                           \"WHERE eventType = 'purchase' or eventType = 'app_open' or eventType = 'app_close' \"\n",
    "                           \"ORDER BY userId, eventTime\")\n",
    "clickstream_df.createOrReplaceTempView('clickstream')\n",
    "\n",
    "purchases_df = spark.read.csv(path='input_csv_datasets/purchases_sample.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickstream_df.createOrReplaceTempView('clickstream')\n",
    "purchases_df.createOrReplaceTempView('purchases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 Load from parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('input_dataset/'):\n",
    "    shutil.rmtree('input_dataset/')\n",
    "    \n",
    "clickstream_df.write.partitionBy('eventTime').parquet('input_dataset/clickstream.parquet')\n",
    "purchases_df.write.partitionBy('purchaseTime').parquet('input_dataset/purchases.parquet')\n",
    "\n",
    "clickstream_df = spark.read.parquet('input_dataset/clickstream.parquet')\n",
    "purchases_df = spark.read.parquet('input_dataset/purchases.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining UDFs for parsing the attributes column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_campaign_id(row):\n",
    "    try:\n",
    "        row_json = json.loads(row.replace(\"{{\", '{').replace(\"}}\", '}'))\n",
    "        if 'campaign_id' in row_json:\n",
    "            return row_json['campaign_id']\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_channel_id(row):\n",
    "    try:\n",
    "        row_json = json.loads(row.replace(\"{{\", '{').replace(\"}}\", '}'))\n",
    "        if 'channel_id' in row_json:\n",
    "            return row_json['channel_id']\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def get_purchase_id(row):\n",
    "    try:\n",
    "        row_json = json.loads(row.replace(\"{{\", '{').replace(\"}}\", '}'))\n",
    "        if 'purchase_id' in row_json:\n",
    "            return row_json['purchase_id']\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the attributes column to 3 separate columns named `campaignId`, `channelId` and `purchaseId`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_attributes(df):\n",
    "    get_purchase_id_udf = udf(lambda x: get_purchase_id(x))\n",
    "    get_channel_id_udf = udf(lambda x: get_channel_id(x))\n",
    "    get_campaign_id_udf = udf(lambda x: get_campaign_id(x))\n",
    "    \n",
    "    df = (df.withColumn('campaignId', get_campaign_id_udf(col('attributes')))\n",
    "                      .withColumn('channelId', get_channel_id_udf(col('attributes')))\n",
    "                      .withColumn('purchaseId', get_purchase_id_udf(col('attributes')))\n",
    "                      .drop(col('attributes')))\n",
    "    df.createOrReplaceTempView('clickstream')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------------------+---------+----------+------------+----------+\n",
      "|userId|eventId|          eventTime|eventType|campaignId|   channelId|purchaseId|\n",
      "+------+-------+-------------------+---------+----------+------------+----------+\n",
      "|    u1|  u1_e1| 2019-01-01 0:00:00| app_open|      cmp1|  Google Ads|      null|\n",
      "|    u1|  u1_e6| 2019-01-01 0:01:00| purchase|      null|        null|        p1|\n",
      "|    u1|  u1_e7| 2019-01-01 0:02:00|app_close|      null|        null|      null|\n",
      "|    u2|  u2_e1| 2019-01-01 0:00:00| app_open|      cmp1|  Yandex Ads|      null|\n",
      "|    u2|  u2_e4| 2019-01-01 0:03:00| purchase|      null|        null|        p2|\n",
      "|    u2|  u2_e5| 2019-01-01 0:04:00|app_close|      null|        null|      null|\n",
      "|    u2|  u2_e6| 2019-01-02 0:00:00| app_open|      cmp2|  Yandex Ads|      null|\n",
      "|    u2| u2_e10| 2019-01-02 0:04:00|app_close|      null|        null|      null|\n",
      "|    u3|  u3_e1| 2019-01-01 0:00:00| app_open|      cmp2|Facebook Ads|      null|\n",
      "|    u3|  u3_e4| 2019-01-01 0:02:00|app_close|      null|        null|      null|\n",
      "|    u3|  u3_e5| 2019-01-01 1:11:11| app_open|      cmp1|  Google Ads|      null|\n",
      "|    u3|  u3_e8| 2019-01-01 1:12:00| purchase|      null|        null|        p3|\n",
      "|    u3|  u3_e9| 2019-01-01 1:12:30|app_close|      null|        null|      null|\n",
      "|    u3| u3_e19|2019-01-02 13:00:10| app_open|      cmp2|  Yandex Ads|      null|\n",
      "|    u3| u3_e22|2019-01-02 13:03:00| purchase|      null|        null|        p6|\n",
      "|    u3| u3_e23|2019-01-02 13:06:00|app_close|      null|        null|      null|\n",
      "|    u3| u3_e10| 2019-01-02 2:00:00| app_open|      cmp2|  Yandex Ads|      null|\n",
      "|    u3| u3_e14| 2019-01-02 2:13:00| purchase|      null|        null|        p4|\n",
      "|    u3| u3_e17| 2019-01-02 2:15:00| purchase|      null|        null|        p5|\n",
      "|    u3| u3_e18| 2019-01-02 2:15:40|app_close|      null|        null|      null|\n",
      "+------+-------+-------------------+---------+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clickstream_df = parse_attributes(clickstream_df)\n",
    "clickstream_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare session dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_df():\n",
    "    sessions_df = spark.sql(\"SELECT userId, eventId, eventType, eventTime, campaignId, channelId \"\n",
    "                            \"FROM clickstream \"\n",
    "                            \"WHERE eventType = 'app_open' or eventType = 'app_close'\")\n",
    "    sessions_df.createOrReplaceTempView('sessions')\n",
    "    \n",
    "    sessions_df = spark.sql(\"\"\"SELECT * FROM \n",
    "                        (SELECT userId, eventTime as start_session, \n",
    "                        LEAD (eventTime, 1) OVER (PARTITION BY userId ORDER BY eventTime) as end_session, \n",
    "                        eventType, campaignId, channelId, eventId as session_id\n",
    "                        FROM sessions ORDER BY userId, start_session) \n",
    "                        WHERE eventType = 'app_open';\"\"\")\n",
    "    sessions_df.createOrReplaceTempView('sessions')\n",
    "    return sessions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+---------+----------+------------+----------+\n",
      "|userId|      start_session|        end_session|eventType|campaignId|   channelId|session_id|\n",
      "+------+-------------------+-------------------+---------+----------+------------+----------+\n",
      "|    u1| 2019-01-01 0:00:00| 2019-01-01 0:02:00| app_open|      cmp1|  Google Ads|     u1_e1|\n",
      "|    u2| 2019-01-01 0:00:00| 2019-01-01 0:04:00| app_open|      cmp1|  Yandex Ads|     u2_e1|\n",
      "|    u2| 2019-01-02 0:00:00| 2019-01-02 0:04:00| app_open|      cmp2|  Yandex Ads|     u2_e6|\n",
      "|    u3| 2019-01-01 0:00:00| 2019-01-01 0:02:00| app_open|      cmp2|Facebook Ads|     u3_e1|\n",
      "|    u3| 2019-01-01 1:11:11| 2019-01-01 1:12:30| app_open|      cmp1|  Google Ads|     u3_e5|\n",
      "|    u3|2019-01-02 13:00:10|2019-01-02 13:06:00| app_open|      cmp2|  Yandex Ads|    u3_e19|\n",
      "|    u3| 2019-01-02 2:00:00| 2019-01-02 2:15:40| app_open|      cmp2|  Yandex Ads|    u3_e10|\n",
      "+------+-------------------+-------------------+---------+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_session_df()\n",
    "spark.sql(\"SELECT * FROM sessions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grep only purchases from clickstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_sessions_for_purchases():\n",
    "    df = spark.sql(\"SELECT * FROM clickstream WHERE eventType = 'purchase'\")\n",
    "    df.createOrReplaceTempView('clickstream_purchases')\n",
    "    clicks_sessions_df = spark.sql(\"SELECT clicks.userId, clicks.eventId, sessions.campaignId, sessions.channelId, clicks.purchaseId, sessions.session_id \"\n",
    "                                   \"FROM clickstream_purchases clicks JOIN sessions \"\n",
    "                                   \"ON (clicks.userId = sessions.userId AND sessions.start_session < clicks.eventTime AND sessions.end_session > clicks.eventTime) \"\n",
    "                                   \"ORDER BY userId\")\n",
    "    clicks_sessions_df.createOrReplaceTempView('clicks_sessions')\n",
    "    return clicks_sessions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide session for purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+----------+----------+\n",
      "|userId|eventId|campaignId| channelId|purchaseId|session_id|\n",
      "+------+-------+----------+----------+----------+----------+\n",
      "|    u1|  u1_e6|      cmp1|Google Ads|        p1|     u1_e1|\n",
      "|    u2|  u2_e4|      cmp1|Yandex Ads|        p2|     u2_e1|\n",
      "|    u3|  u3_e8|      cmp1|Google Ads|        p3|     u3_e5|\n",
      "|    u3| u3_e17|      cmp2|Yandex Ads|        p5|    u3_e10|\n",
      "|    u3| u3_e22|      cmp2|Yandex Ads|        p6|    u3_e19|\n",
      "|    u3| u3_e14|      cmp2|Yandex Ads|        p4|    u3_e10|\n",
      "+------+-------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "provide_sessions_for_purchases()\n",
    "spark.sql(\"SELECT * FROM clicks_sessions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------+-----------+----------+----------+----------+\n",
      "|purchaseId|       purchaseTime|billingCost|isConfirmed|campaignId| channelId|session_id|\n",
      "+----------+-------------------+-----------+-----------+----------+----------+----------+\n",
      "|        p3| 2019-01-01 1:12:15|        300|      FALSE|      cmp1|Google Ads|     u3_e5|\n",
      "|        p6|2019-01-02 13:03:00|         99|      FALSE|      cmp2|Yandex Ads|    u3_e19|\n",
      "|        p5| 2019-01-01 2:15:05|         75|       TRUE|      cmp2|Yandex Ads|    u3_e10|\n",
      "|        p4| 2019-01-01 2:13:05|       50.2|       TRUE|      cmp2|Yandex Ads|    u3_e10|\n",
      "|        p1| 2019-01-01 0:01:05|      100.5|       TRUE|      cmp1|Google Ads|     u1_e1|\n",
      "|        p2| 2019-01-01 0:03:10|        200|       TRUE|      cmp1|Yandex Ads|     u2_e1|\n",
      "+----------+-------------------+-----------+-----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "projection = spark.sql(\"SELECT purchases.*, clicks_sessions.campaignId, clicks_sessions.channelId, clicks_sessions.session_id \"\n",
    "                       \"FROM clicks_sessions JOIN purchases ON (clicks_sessions.purchaseId = purchases.purchaseId)\")\n",
    "\n",
    "projection.createOrReplaceTempView('projection')\n",
    "projection.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: . Save output for Task #1 as parquet as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_parquet(df, task, name):\n",
    "    if os.path.exists(task):\n",
    "        shutil.rmtree(task)\n",
    "    \n",
    "    projection.write.parquet(task + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_parquet(projection, 'task1/', 'projection.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Calculate Marketing Campaigns And Channels Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What are the Top 10 marketing campaigns that bring the biggest revenue (based on billingCost of confirmed purchases)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|campaignId|revenue|\n",
      "+----------+-------+\n",
      "|      cmp1|  300.5|\n",
      "|      cmp2|  125.2|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_1_df = spark.sql(\"SELECT campaignId, sum(billingCost) as revenue \"\n",
    "          \"FROM projection \"\n",
    "          \"WHERE isConfirmed = 'TRUE' \"\n",
    "          \"GROUP BY campaignId \"\n",
    "          \"ORDER BY revenue DESC LIMIT 10;\")\n",
    "\n",
    "save_as_parquet(task2_1_df, 'task2_1/', 'top_campaign.parquet')\n",
    "task2_1_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 What is the most popular (i.e. Top) channel that drives the highest amount of unique sessions (engagements)  with the App in each campaign?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------------+\n",
      "| channelId|campaignId|amount_of_unique_session|\n",
      "+----------+----------+------------------------+\n",
      "|Yandex Ads|      cmp2|                       2|\n",
      "|Google Ads|      cmp1|                       2|\n",
      "|Yandex Ads|      cmp1|                       1|\n",
      "+----------+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_2_df = spark.sql(\"SELECT channelId, campaignId, count(distinct session_id) as amount_of_unique_session \"\n",
    "          \"FROM projection \"\n",
    "          \"GROUP BY channelId, campaignId \"\n",
    "          \"ORDER BY amount_of_unique_session DESC\")\n",
    "save_as_parquet(task2_2_df, 'task2_2/', 'top_channel.parquet')\n",
    "task2_2_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Calculate metrics from Task #2 for different time periods: September 2020, 2020-11-11\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|campaignId|revenue|\n",
      "+----------+-------+\n",
      "|      cmp1|  300.5|\n",
      "|      cmp2|  125.2|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date = '2019-01'\n",
    "spark.sql(\"SELECT campaignId, sum(billingCost) as revenue \"\n",
    "          \"FROM projection \"\n",
    "          f\"WHERE isConfirmed = 'TRUE' AND purchaseTime like '%{date}%' \"\n",
    "          \"GROUP BY campaignId ORDER BY revenue DESC;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving query plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2019-01'\n",
    "task3_plan = spark.sql(\"SELECT campaignId, sum(billingCost) as revenue \"\n",
    "                       \"FROM projection \"\n",
    "                       f\"WHERE isConfirmed = 'TRUE' AND purchaseTime like '%{date}%' \"\n",
    "                       \"GROUP BY campaignId ORDER BY revenue DESC LIMIT 10;\")._jdf.queryExecution().toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('task3_plan.MD', 'w+') as f:\n",
    "    f.write(task3_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_parquet_and_csv_performance():\n",
    "    start = time()\n",
    "    clickstream_df = spark.read.csv(sep=r'\\t', path='input_csv_datasets/mobile-app-clickstream_sample.tsv', header=True)\n",
    "    print(f\"Reading from CSV:\\t{time() - start}\")\n",
    "    \n",
    "    if os.path.exists('input_dataset/'):\n",
    "        shutil.rmtree('input_dataset/')\n",
    "    clickstream_df.write.partitionBy('eventTime').parquet('input_dataset/clickstream.parquet')\n",
    "    \n",
    "    start = time()\n",
    "    spark.read.parquet('input_dataset/clickstream.parquet')\n",
    "    print(f\"Reading from Parquet:\\t{time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from CSV:\t0.1139211654663086\n",
      "Reading from Parquet:\t0.49912595748901367\n"
     ]
    }
   ],
   "source": [
    "compare_parquet_and_csv_performance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
