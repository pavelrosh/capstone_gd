{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import MapType\n",
    "from pyspark.sql.functions import udf, col, array, lit\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import shutil\n",
    "from time import time\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Build Purchases Attribution Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.31.213:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10cfa5b20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickstream_df = spark.read.csv(sep=r'\\t', path='mobile-app-clickstream_sample.tsv', header=True)\n",
    "clickstream_df.createOrReplaceTempView('clickstream')\n",
    "clickstream_df = spark.sql(\"SELECT * FROM clickstream \"\n",
    "                           \"WHERE eventType = 'purchase' or eventType = 'app_open' or eventType = 'app_close' \"\n",
    "                           \"ORDER BY userId, eventTime\")\n",
    "clickstream_df.createOrReplaceTempView('clickstream')\n",
    "\n",
    "purchases_df = spark.read.csv(path='purchases_sample.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickstream_df.createOrReplaceTempView('clickstream')\n",
    "purchases_df.createOrReplaceTempView('purchases')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 Load from parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('input_dataset/'):\n",
    "    shutil.rmtree('input_dataset/')\n",
    "    \n",
    "clickstream_df.write.partitionBy('eventTime').parquet('input_dataset/clickstream.parquet')\n",
    "purchases_df.write.partitionBy('purchaseTime').parquet('input_dataset/purchases.parquet')\n",
    "\n",
    "clickstream_df = spark.read.parquet('input_dataset/clickstream.parquet')\n",
    "purchases_df = spark.read.parquet('input_dataset/purchases.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining UDFs for parsing the attributes column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_campaign_id(row):\n",
    "    try:\n",
    "        row_json = json.loads(row.replace(\"{{\", '{').replace(\"}}\", '}'))\n",
    "        if 'campaign_id' in row_json:\n",
    "            return row_json['campaign_id']\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_channel_id(row):\n",
    "    try:\n",
    "        row_json = json.loads(row.replace(\"{{\", '{').replace(\"}}\", '}'))\n",
    "        if 'channel_id' in row_json:\n",
    "            return row_json['channel_id']\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def get_purchase_id(row):\n",
    "    try:\n",
    "        row_json = json.loads(row.replace(\"{{\", '{').replace(\"}}\", '}'))\n",
    "        if 'purchase_id' in row_json:\n",
    "            return row_json['purchase_id']\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "get_purchase_id_udf = udf(lambda x: get_purchase_id(x))\n",
    "get_channel_id_udf = udf(lambda x: get_channel_id(x))\n",
    "get_campaign_id_udf = udf(lambda x: get_campaign_id(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the attributes column to 3 separate columns named `campaignId`, `channelId` and `purchaseId`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_attributes(df):\n",
    "    df = (df.withColumn('campaignId', get_campaign_id_udf(col('attributes')))\n",
    "                      .withColumn('channelId', get_channel_id_udf(col('attributes')))\n",
    "                      .withColumn('purchaseId', get_purchase_id_udf(col('attributes')))\n",
    "                      .drop(col('attributes')))\n",
    "    df.createOrReplaceTempView('clickstream')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------------------+---------+----------+------------+----------+\n",
      "|userId|eventId|          eventTime|eventType|campaignId|   channelId|purchaseId|\n",
      "+------+-------+-------------------+---------+----------+------------+----------+\n",
      "|    u1|  u1_e1| 2019-01-01 0:00:00| app_open|      cmp1|  Google Ads|      null|\n",
      "|    u1|  u1_e6| 2019-01-01 0:01:00| purchase|      null|        null|        p1|\n",
      "|    u1|  u1_e7| 2019-01-01 0:02:00|app_close|      null|        null|      null|\n",
      "|    u2|  u2_e1| 2019-01-01 0:00:00| app_open|      cmp1|  Yandex Ads|      null|\n",
      "|    u2|  u2_e4| 2019-01-01 0:03:00| purchase|      null|        null|        p2|\n",
      "|    u2|  u2_e5| 2019-01-01 0:04:00|app_close|      null|        null|      null|\n",
      "|    u2|  u2_e6| 2019-01-02 0:00:00| app_open|      cmp2|  Yandex Ads|      null|\n",
      "|    u2| u2_e10| 2019-01-02 0:04:00|app_close|      null|        null|      null|\n",
      "|    u3|  u3_e1| 2019-01-01 0:00:00| app_open|      cmp2|Facebook Ads|      null|\n",
      "|    u3|  u3_e4| 2019-01-01 0:02:00|app_close|      null|        null|      null|\n",
      "|    u3|  u3_e5| 2019-01-01 1:11:11| app_open|      cmp1|  Google Ads|      null|\n",
      "|    u3|  u3_e8| 2019-01-01 1:12:00| purchase|      null|        null|        p3|\n",
      "|    u3|  u3_e9| 2019-01-01 1:12:30|app_close|      null|        null|      null|\n",
      "|    u3| u3_e19|2019-01-02 13:00:10| app_open|      cmp2|  Yandex Ads|      null|\n",
      "|    u3| u3_e22|2019-01-02 13:03:00| purchase|      null|        null|        p6|\n",
      "|    u3| u3_e23|2019-01-02 13:06:00|app_close|      null|        null|      null|\n",
      "|    u3| u3_e10| 2019-01-02 2:00:00| app_open|      cmp2|  Yandex Ads|      null|\n",
      "|    u3| u3_e14| 2019-01-02 2:13:00| purchase|      null|        null|        p4|\n",
      "|    u3| u3_e17| 2019-01-02 2:15:00| purchase|      null|        null|        p5|\n",
      "|    u3| u3_e18| 2019-01-02 2:15:40|app_close|      null|        null|      null|\n",
      "+------+-------+-------------------+---------+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clickstream_df = parse_attributes(clickstream_df)\n",
    "clickstream_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare session dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_session_df():\n",
    "    sessions_df = spark.sql(\"SELECT userId, eventId, eventType, eventTime, campaignId, channelId \"\n",
    "                            \"FROM clickstream \"\n",
    "                            \"WHERE eventType = 'app_open' or eventType = 'app_close'\")\n",
    "    sessions_df.createOrReplaceTempView('sessions')\n",
    "    \n",
    "    sessions_df = spark.sql(\"\"\"SELECT * FROM \n",
    "                        (SELECT userId, eventTime as start_session, \n",
    "                        LEAD (eventTime, 1) OVER (PARTITION BY userId ORDER BY eventTime) as end_session, \n",
    "                        eventType, campaignId, channelId, eventId as session_id\n",
    "                        FROM sessions ORDER BY userId, start_session) \n",
    "                        WHERE eventType = 'app_open';\"\"\")\n",
    "    sessions_df.createOrReplaceTempView('sessions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+---------+----------+------------+----------+\n",
      "|userId|      start_session|        end_session|eventType|campaignId|   channelId|session_id|\n",
      "+------+-------------------+-------------------+---------+----------+------------+----------+\n",
      "|    u1| 2019-01-01 0:00:00| 2019-01-01 0:02:00| app_open|      cmp1|  Google Ads|     u1_e1|\n",
      "|    u2| 2019-01-01 0:00:00| 2019-01-01 0:04:00| app_open|      cmp1|  Yandex Ads|     u2_e1|\n",
      "|    u2| 2019-01-02 0:00:00| 2019-01-02 0:04:00| app_open|      cmp2|  Yandex Ads|     u2_e6|\n",
      "|    u3| 2019-01-01 0:00:00| 2019-01-01 0:02:00| app_open|      cmp2|Facebook Ads|     u3_e1|\n",
      "|    u3| 2019-01-01 1:11:11| 2019-01-01 1:12:30| app_open|      cmp1|  Google Ads|     u3_e5|\n",
      "|    u3|2019-01-02 13:00:10|2019-01-02 13:06:00| app_open|      cmp2|  Yandex Ads|    u3_e19|\n",
      "|    u3| 2019-01-02 2:00:00| 2019-01-02 2:15:40| app_open|      cmp2|  Yandex Ads|    u3_e10|\n",
      "+------+-------------------+-------------------+---------+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_session_df()\n",
    "spark.sql(\"SELECT * FROM sessions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grep only purchases from clickstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def provide_sessions_for_purchases():\n",
    "    df = spark.sql(\"SELECT * FROM clickstream WHERE eventType = 'purchase'\")\n",
    "    df.createOrReplaceTempView('clickstream_purchases')\n",
    "    clicks_sessions_df = spark.sql(\"SELECT clicks.userId, clicks.eventId, sessions.campaignId, sessions.channelId, clicks.purchaseId, sessions.session_id \"\n",
    "                                   \"FROM clickstream_purchases clicks JOIN sessions \"\n",
    "                                   \"ON (clicks.userId = sessions.userId AND sessions.start_session < clicks.eventTime AND sessions.end_session > clicks.eventTime) \"\n",
    "                                   \"ORDER BY userId\")\n",
    "    clicks_sessions_df.createOrReplaceTempView('clicks_sessions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide session for purchases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+----------+----------+\n",
      "|userId|eventId|campaignId| channelId|purchaseId|session_id|\n",
      "+------+-------+----------+----------+----------+----------+\n",
      "|    u1|  u1_e6|      cmp1|Google Ads|        p1|     u1_e1|\n",
      "|    u2|  u2_e4|      cmp1|Yandex Ads|        p2|     u2_e1|\n",
      "|    u3|  u3_e8|      cmp1|Google Ads|        p3|     u3_e5|\n",
      "|    u3| u3_e17|      cmp2|Yandex Ads|        p5|    u3_e10|\n",
      "|    u3| u3_e22|      cmp2|Yandex Ads|        p6|    u3_e19|\n",
      "|    u3| u3_e14|      cmp2|Yandex Ads|        p4|    u3_e10|\n",
      "+------+-------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "provide_sessions_for_purchases()\n",
    "spark.sql(\"SELECT * FROM clicks_sessions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------+-----------+----------+----------+----------+\n",
      "|purchaseId|       purchaseTime|billingCost|isConfirmed|campaignId| channelId|session_id|\n",
      "+----------+-------------------+-----------+-----------+----------+----------+----------+\n",
      "|        p3| 2019-01-01 1:12:15|        300|      FALSE|      cmp1|Google Ads|     u3_e5|\n",
      "|        p6|2019-01-02 13:03:00|         99|      FALSE|      cmp2|Yandex Ads|    u3_e19|\n",
      "|        p5| 2019-01-01 2:15:05|         75|       TRUE|      cmp2|Yandex Ads|    u3_e10|\n",
      "|        p4| 2019-01-01 2:13:05|       50.2|       TRUE|      cmp2|Yandex Ads|    u3_e10|\n",
      "|        p1| 2019-01-01 0:01:05|      100.5|       TRUE|      cmp1|Google Ads|     u1_e1|\n",
      "|        p2| 2019-01-01 0:03:10|        200|       TRUE|      cmp1|Yandex Ads|     u2_e1|\n",
      "+----------+-------------------+-----------+-----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "projection = spark.sql(\"SELECT purchases.*, clicks_sessions.campaignId, clicks_sessions.channelId, clicks_sessions.session_id \"\n",
    "                       \"FROM clicks_sessions JOIN purchases ON (clicks_sessions.purchaseId = purchases.purchaseId)\")\n",
    "\n",
    "projection.createOrReplaceTempView('projection')\n",
    "projection.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: . Save output for Task #1 as parquet as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_parquet(df, task, name):\n",
    "    if os.path.exists(task):\n",
    "        shutil.rmtree(task)\n",
    "    \n",
    "    projection.write.parquet(task + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_parquet(projection, 'task1/', 'projection.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Calculate Marketing Campaigns And Channels Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 What are the Top 10 marketing campaigns that bring the biggest revenue (based on billingCost of confirmed purchases)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|campaignId|revenue|\n",
      "+----------+-------+\n",
      "|      cmp1|  300.5|\n",
      "|      cmp2|  125.2|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_1_df = spark.sql(\"SELECT campaignId, sum(billingCost) as revenue \"\n",
    "          \"FROM projection \"\n",
    "          \"WHERE isConfirmed = 'TRUE' \"\n",
    "          \"GROUP BY campaignId \"\n",
    "          \"ORDER BY revenue DESC LIMIT 10;\")\n",
    "\n",
    "save_as_parquet(task2_1_df, 'task2_1/', 'top_campaign.parquet')\n",
    "task2_1_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 What is the most popular (i.e. Top) channel that drives the highest amount of unique sessions (engagements)  with the App in each campaign?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------------+\n",
      "| channelId|campaignId|amount_of_unique_session|\n",
      "+----------+----------+------------------------+\n",
      "|Yandex Ads|      cmp2|                       2|\n",
      "|Google Ads|      cmp1|                       2|\n",
      "|Yandex Ads|      cmp1|                       1|\n",
      "+----------+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_2_df = spark.sql(\"SELECT channelId, campaignId, count(distinct session_id) as amount_of_unique_session \"\n",
    "          \"FROM projection \"\n",
    "          \"GROUP BY channelId, campaignId \"\n",
    "          \"ORDER BY amount_of_unique_session DESC\")\n",
    "save_as_parquet(task2_2_df, 'task2_2/', 'top_channel.parquet')\n",
    "task2_2_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Calculate metrics from Task #2 for different time periods: September 2020, 2020-11-11\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|campaignId|revenue|\n",
      "+----------+-------+\n",
      "|      cmp1|  300.5|\n",
      "|      cmp2|  125.2|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date = '2019-01'\n",
    "spark.sql(\"SELECT campaignId, sum(billingCost) as revenue \"\n",
    "          \"FROM projection \"\n",
    "          f\"WHERE isConfirmed = 'TRUE' AND purchaseTime like '%{date}%' \"\n",
    "          \"GROUP BY campaignId ORDER BY revenue DESC;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving query plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Table or view not found: projection; line 1 pos 52;\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Sort ['revenue DESC NULLS LAST], true\n      +- 'Aggregate ['campaignId], ['campaignId, 'sum('billingCost) AS revenue#633]\n         +- 'Filter (('isConfirmed = TRUE) AND 'purchaseTime LIKE %2019-01%)\n            +- 'UnresolvedRelation [projection]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-6365323b32fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2019-01'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m task3_plan = spark.sql(\"SELECT campaignId, sum(billingCost) as revenue \"\n\u001b[0m\u001b[1;32m      3\u001b[0m                        \u001b[0;34m\"FROM projection \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                        \u001b[0;34mf\"WHERE isConfirmed = 'TRUE' AND purchaseTime like '%{date}%' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                        \"GROUP BY campaignId ORDER BY revenue DESC LIMIT 10;\")._jdf.queryExecution().toString()\n",
      "\u001b[0;32m~/PycharmProjects/capstone/venv/lib/python3.8/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery)\u001b[0m\n\u001b[1;32m    647\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'row3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \"\"\"\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/capstone/venv/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/capstone/venv/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/capstone/venv/lib/python3.8/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Table or view not found: projection; line 1 pos 52;\n'GlobalLimit 10\n+- 'LocalLimit 10\n   +- 'Sort ['revenue DESC NULLS LAST], true\n      +- 'Aggregate ['campaignId], ['campaignId, 'sum('billingCost) AS revenue#633]\n         +- 'Filter (('isConfirmed = TRUE) AND 'purchaseTime LIKE %2019-01%)\n            +- 'UnresolvedRelation [projection]\n"
     ]
    }
   ],
   "source": [
    "date = '2019-01'\n",
    "task3_plan = spark.sql(\"SELECT campaignId, sum(billingCost) as revenue \"\n",
    "                       \"FROM projection \"\n",
    "                       f\"WHERE isConfirmed = 'TRUE' AND purchaseTime like '%{date}%' \"\n",
    "                       \"GROUP BY campaignId ORDER BY revenue DESC LIMIT 10;\")._jdf.queryExecution().toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('task3_plan.MD', 'w+') as f:\n",
    "    f.write(task3_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_parquet_and_csv_performance():\n",
    "    start = time()\n",
    "    clickstream_df = spark.read.csv(sep=r'\\t', path='mobile-app-clickstream_sample.tsv', header=True)\n",
    "    print(f\"Reading from CSV:\\t{time() - start}\")\n",
    "    \n",
    "    if os.path.exists('input_dataset/'):\n",
    "        shutil.rmtree('input_dataset/')\n",
    "    clickstream_df.write.partitionBy('eventTime').parquet('input_dataset/clickstream.parquet')\n",
    "    \n",
    "    start = time()\n",
    "    spark.read.parquet('input_dataset/clickstream.parquet')\n",
    "    print(f\"Reading from Parquet:\\t{time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from CSV:\t0.1065070629119873\n",
      "Reading from Parquet:\t0.2256178855895996\n"
     ]
    }
   ],
   "source": [
    "compare_parquet_and_csv_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapstoneTest(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def create_testing_pyspark_session(cls):\n",
    "        return (SparkSession.builder.master('local[2]').appName('my-local-testing-pyspark-context').getOrCreate())\n",
    "    \n",
    "    def test_loading_data(self):\n",
    "        clickstream_df = spark.read.csv(sep=r'\\t', path='mobile-app-clickstream_sample.tsv', header=True)\n",
    "        purchases_df = spark.read.csv('purchases_sample.csv', header=True)\n",
    "        \n",
    "        assert clickstream_df.count() == 39\n",
    "        assert purchases_df.count() == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_loading_data (__main__.CapstoneTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.326s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x106ae6520>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
