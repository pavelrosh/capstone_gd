{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import MapType\n",
    "from pyspark.sql.functions import udf, col, array, lit\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import shutil\n",
    "from time import time\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.31.213:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Build Purchases Attribution Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickstream_df = spark.read.csv(sep=r'\\t', path='mobile-app-clickstream_sample.tsv', header=True)\n",
    "clickstream_df.createOrReplaceTempView('clickstream')\n",
    "clickstream_df = spark.sql(\"SELECT * FROM clickstream where eventType = 'purchase' or eventType = 'app_open' or eventType = 'app_close' ORDER BY userId, eventTime\")\n",
    "\n",
    "purchases_df = spark.read.csv('purchases_sample.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickstream_df.createOrReplaceTempView('clickstream')\n",
    "purchases_df.createOrReplaceTempView('purchases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clickstream_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 Load from parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('input_dataset/'):\n",
    "    shutil.rmtree('input_dataset/')\n",
    "    \n",
    "clickstream_df.write.partitionBy('eventTime').parquet('input_dataset/clickstream.parquet')\n",
    "purchases_df.write.partitionBy('purchaseTime').parquet('input_dataset/purchases.parquet')\n",
    "\n",
    "clickstream_df = spark.read.parquet('input_dataset/clickstream.parquet')\n",
    "purchases_df = spark.read.parquet('input_dataset/purchases.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining UDFs for parsing the attributes column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_campaign_id(row):\n",
    "    try:\n",
    "        row_json = json.loads(row.replace(\"{{\", '{').replace(\"}}\", '}'))\n",
    "        if 'campaign_id' in row_json:\n",
    "            return row_json['campaign_id']\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_channel_id(row):\n",
    "    try:\n",
    "        row_json = json.loads(row.replace(\"{{\", '{').replace(\"}}\", '}'))\n",
    "        if 'channel_id' in row_json:\n",
    "            return row_json['channel_id']\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def get_purchase_id(row):\n",
    "    try:\n",
    "        row_json = json.loads(row.replace(\"{{\", '{').replace(\"}}\", '}'))\n",
    "        if 'purchase_id' in row_json:\n",
    "            return row_json['purchase_id']\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "get_purchase_id_udf = udf(lambda x: get_purchase_id(x))\n",
    "get_channel_id_udf = udf(lambda x: get_channel_id(x))\n",
    "get_campaign_id_udf = udf(lambda x: get_campaign_id(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the attributes column to 3 separate columns named `campaignId`, `channelId` and `purchaseId`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clickstream_df = clickstream_df.withColumn('campaignId', get_campaign_id_udf(col('attributes')))\n",
    "clickstream_df = clickstream_df.withColumn('channelId', get_channel_id_udf(col('attributes')))\n",
    "clickstream_df = clickstream_df.withColumn('purchaseId', get_purchase_id_udf(col('attributes')))\n",
    "clickstream_df = clickstream_df.drop(col('attributes'))\n",
    "clickstream_df.createOrReplaceTempView('clickstream')\n",
    "\n",
    "# clickstream_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grep sessions from dataframe as app_open and app_close rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_df = spark.sql(\"SELECT userId, eventId, eventType, eventTime, campaignId, channelId FROM clickstream where eventType = 'app_open' or eventType = 'app_close'\")\n",
    "sessions_df.createOrReplaceTempView('sessions')\n",
    "# session_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding `session_end` columnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+---------+----------+------------+----------+\n",
      "|userId|      start_session|        end_session|eventType|campaignId|   channelId|session_id|\n",
      "+------+-------------------+-------------------+---------+----------+------------+----------+\n",
      "|    u1| 2019-01-01 0:00:00| 2019-01-01 0:02:00| app_open|      cmp1|  Google Ads|     u1_e1|\n",
      "|    u2| 2019-01-01 0:00:00| 2019-01-01 0:04:00| app_open|      cmp1|  Yandex Ads|     u2_e1|\n",
      "|    u2| 2019-01-02 0:00:00| 2019-01-02 0:04:00| app_open|      cmp2|  Yandex Ads|     u2_e6|\n",
      "|    u3| 2019-01-01 0:00:00| 2019-01-01 0:02:00| app_open|      cmp2|Facebook Ads|     u3_e1|\n",
      "|    u3| 2019-01-01 1:11:11| 2019-01-01 1:12:30| app_open|      cmp1|  Google Ads|     u3_e5|\n",
      "|    u3|2019-01-02 13:00:10|2019-01-02 13:06:00| app_open|      cmp2|  Yandex Ads|    u3_e19|\n",
      "|    u3| 2019-01-02 2:00:00| 2019-01-02 2:15:40| app_open|      cmp2|  Yandex Ads|    u3_e10|\n",
      "+------+-------------------+-------------------+---------+----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sessions_df = spark.sql(\"\"\"SELECT * FROM \n",
    "                        (SELECT userId, eventTime as start_session, \n",
    "                        LEAD (eventTime, 1) OVER (PARTITION BY userId ORDER BY eventTime) as end_session, \n",
    "                        eventType, campaignId, channelId, eventId as session_id\n",
    "                        FROM sessions ORDER BY userId, start_session) \n",
    "                        WHERE eventType = 'app_open';\"\"\")\n",
    "sessions_df.createOrReplaceTempView('sessions')\n",
    "\n",
    "sessions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grep only purchases from clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+---------+-------------------+----------+---------+----------+\n",
      "|userId|eventId|eventType|          eventTime|campaignId|channelId|purchaseId|\n",
      "+------+-------+---------+-------------------+----------+---------+----------+\n",
      "|    u3| u3_e22| purchase|2019-01-02 13:03:00|      null|     null|        p6|\n",
      "|    u3| u3_e14| purchase| 2019-01-02 2:13:00|      null|     null|        p4|\n",
      "|    u3| u3_e17| purchase| 2019-01-02 2:15:00|      null|     null|        p5|\n",
      "|    u3|  u3_e8| purchase| 2019-01-01 1:12:00|      null|     null|        p3|\n",
      "|    u2|  u2_e4| purchase| 2019-01-01 0:03:00|      null|     null|        p2|\n",
      "|    u1|  u1_e6| purchase| 2019-01-01 0:01:00|      null|     null|        p1|\n",
      "+------+-------+---------+-------------------+----------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clickstream_purchases = clickstream_df.where(col('eventType') == 'purchase')\n",
    "clickstream_purchases.createOrReplaceTempView('clickstream_purchases')\n",
    "clickstream_purchases.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting sessions as rows between `start_session` and `end_session` timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+----------+----------+----------+\n",
      "|userId|eventId|campaignId| channelId|purchaseId|session_id|\n",
      "+------+-------+----------+----------+----------+----------+\n",
      "|    u1|  u1_e6|      cmp1|Google Ads|        p1|     u1_e1|\n",
      "|    u2|  u2_e4|      cmp1|Yandex Ads|        p2|     u2_e1|\n",
      "|    u3|  u3_e8|      cmp1|Google Ads|        p3|     u3_e5|\n",
      "|    u3| u3_e22|      cmp2|Yandex Ads|        p6|    u3_e19|\n",
      "|    u3| u3_e14|      cmp2|Yandex Ads|        p4|    u3_e10|\n",
      "|    u3| u3_e17|      cmp2|Yandex Ads|        p5|    u3_e10|\n",
      "+------+-------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clicks_sessions_df = spark.sql(\"SELECT clicks.userId, clicks.eventId, sessions.campaignId, sessions.channelId, clicks.purchaseId, sessions.session_id \"\n",
    "          \"FROM clickstream_purchases clicks JOIN sessions \"\n",
    "          \"ON (clicks.userId = sessions.userId AND sessions.start_session < clicks.eventTime AND sessions.end_session > clicks.eventTime) ORDER BY userId\")\n",
    "clicks_sessions_df.createOrReplaceTempView('clicks_sessions')\n",
    "clicks_sessions_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing projection as join between purchases and sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-----------+-----------+----------+----------+----------+\n",
      "|purchaseId|       purchaseTime|billingCost|isConfirmed|campaignId| channelId|session_id|\n",
      "+----------+-------------------+-----------+-----------+----------+----------+----------+\n",
      "|        p3| 2019-01-01 1:12:15|        300|      FALSE|      cmp1|Google Ads|     u3_e5|\n",
      "|        p6|2019-01-02 13:03:00|         99|      FALSE|      cmp2|Yandex Ads|    u3_e19|\n",
      "|        p5| 2019-01-01 2:15:05|         75|       TRUE|      cmp2|Yandex Ads|    u3_e10|\n",
      "|        p4| 2019-01-01 2:13:05|       50.2|       TRUE|      cmp2|Yandex Ads|    u3_e10|\n",
      "|        p1| 2019-01-01 0:01:05|      100.5|       TRUE|      cmp1|Google Ads|     u1_e1|\n",
      "|        p2| 2019-01-01 0:03:10|        200|       TRUE|      cmp1|Yandex Ads|     u2_e1|\n",
      "+----------+-------------------+-----------+-----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "projection = spark.sql(\"SELECT purchases.*, clicks_sessions.campaignId, clicks_sessions.channelId, clicks_sessions.session_id \"\n",
    "                       \"FROM clicks_sessions JOIN purchases ON (clicks_sessions.purchaseId = purchases.purchaseId)\")\n",
    "\n",
    "projection.createOrReplaceTempView('projection')\n",
    "projection.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: . Save output for Task #1 as parquet as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_parquet(df, task, name):\n",
    "    if os.path.exists(task):\n",
    "        shutil.rmtree(task)\n",
    "    \n",
    "    projection.write.parquet(task + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_parquet(projection, 'task1/', 'projection.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Calculate Marketing Campaigns And Channels Statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the Top 10 marketing campaigns that bring the biggest revenue (based on billingCost of confirmed purchases)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|campaignId|revenue|\n",
      "+----------+-------+\n",
      "|      cmp1|  300.5|\n",
      "|      cmp2|  125.2|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_1_df = spark.sql(\"SELECT campaignId, sum(billingCost) as revenue \"\n",
    "          \"FROM projection \"\n",
    "          \"WHERE isConfirmed = 'TRUE' \"\n",
    "          \"GROUP BY campaignId \"\n",
    "          \"ORDER BY revenue DESC LIMIT 10;\")\n",
    "\n",
    "save_as_parquet(task2_1_df, 'task2_1/', 'top_campaign.parquet')\n",
    "task2_1_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the most popular (i.e. Top) channel that drives the highest amount of unique sessions (engagements)  with the App in each campaign?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------------------+\n",
      "| channelId|campaignId|amount_of_unique_session|\n",
      "+----------+----------+------------------------+\n",
      "|Yandex Ads|      cmp2|                       2|\n",
      "|Google Ads|      cmp1|                       2|\n",
      "|Yandex Ads|      cmp1|                       1|\n",
      "+----------+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_2_df = spark.sql(\"SELECT channelId, campaignId, count(distinct session_id) as amount_of_unique_session \"\n",
    "          \"FROM projection \"\n",
    "          \"GROUP BY channelId, campaignId \"\n",
    "          \"ORDER BY amount_of_unique_session DESC\")\n",
    "save_as_parquet(task2_2_df, 'task2_2/', 'top_channel.parquet')\n",
    "task2_2_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.2: Calculate metrics from Task #2 for different time periods: September 2020, 2020-11-11\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|campaignId|revenue|\n",
      "+----------+-------+\n",
      "|      cmp1|  300.5|\n",
      "|      cmp2|  125.2|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date = '2019-01'\n",
    "spark.sql(\"SELECT campaignId, sum(billingCost) as revenue \"\n",
    "          \"FROM projection \"\n",
    "          f\"WHERE isConfirmed = 'TRUE' AND purchaseTime like '%{date}%' \"\n",
    "          \"GROUP BY campaignId ORDER BY revenue DESC;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving query plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "task3_plan = spark.sql(\"SELECT campaignId, sum(billingCost) as revenue FROM projection \"\n",
    "          f\"WHERE isConfirmed = 'TRUE' AND purchaseTime like '%{date}%' GROUP BY campaignId ORDER BY revenue DESC LIMIT 10;\")._jdf.queryExecution().toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('task3_plan.MD', 'w+') as f:\n",
    "    f.write(task3_plan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_parquet_and_csv_performance():\n",
    "    start = time()\n",
    "    clickstream_df = spark.read.csv(sep=r'\\t', path='mobile-app-clickstream_sample.tsv', header=True)\n",
    "    print(f\"Reading from CSV:\\t{time() - start}\")\n",
    "    \n",
    "    if os.path.exists('input_dataset/'):\n",
    "        shutil.rmtree('input_dataset/')\n",
    "    clickstream_df.write.partitionBy('eventTime').parquet('input_dataset/clickstream.parquet')\n",
    "    \n",
    "    start = time()\n",
    "    spark.read.parquet('input_dataset/clickstream.parquet')\n",
    "    print(f\"Reading from Parquet:\\t{time() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from CSV:\t0.1065070629119873\n",
      "Reading from Parquet:\t0.2256178855895996\n"
     ]
    }
   ],
   "source": [
    "compare_parquet_and_csv_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapstoneTest(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def create_testing_pyspark_session(cls):\n",
    "        return (SparkSession.builder.master('local[2]').appName('my-local-testing-pyspark-context').getOrCreate())\n",
    "    \n",
    "    def test_loading_data(self):\n",
    "        clickstream_df = spark.read.csv(sep=r'\\t', path='mobile-app-clickstream_sample.tsv', header=True)\n",
    "        purchases_df = spark.read.csv('purchases_sample.csv', header=True)\n",
    "        \n",
    "        assert clickstream_df.count() == 39\n",
    "        assert purchases_df.count() == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_loading_data (__main__.CapstoneTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.326s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x106ae6520>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
